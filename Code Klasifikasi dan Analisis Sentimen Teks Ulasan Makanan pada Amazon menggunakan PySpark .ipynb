{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQXS-71AMbBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98434d2b-6fe4-4491-aa85-273e821698c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=f6868d8945524a31f5c20397af4aecbe5a2ca8f66c040f6a773f160ca2d7ba03\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark import HiveContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "8Y6YnWvnMvjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SQLContext\n",
        "\n",
        "from pyspark import HiveContext"
      ],
      "metadata": {
        "id": "pc_Onlk6NJX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "sqlContext=HiveContext(sc)"
      ],
      "metadata": {
        "id": "MUEep9IRUcGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a310f81-d0a6-4952-f357-136855cfa500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:720: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 =sqlContext.read.format('com.databricks.spark.csv')\\\n",
        ".options(header='true',inferschema='true')\\\n",
        ".load(\"/content/Reviews.csv\")"
      ],
      "metadata": {
        "id": "lMKX_RSKyvWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdU7RUFKd-z",
        "outputId": "f0c6c587-e064-4c6d-c17c-cc2cbcbb0c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568454"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHHUIZNaJpIe",
        "outputId": "bef9e5dc-0130-45bd-dc26-f861f1544c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
            "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
            "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
            "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n",
            "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n",
            "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n",
            "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n",
            "|  6|B006K2ZZ7K| ADT0SRK1MGOEU|      Twoapennything|                   0|                     0|    4|1342051200|          Nice Taffy|I got a wild hair...|\n",
            "|  7|B006K2ZZ7K|A1SP2KVKFXXRU1|   David C. Sullivan|                   0|                     0|    5|1340150400|Great!  Just as g...|This saltwater ta...|\n",
            "|  8|B006K2ZZ7K|A3JRGQVEQN31IQ|  Pamela G. Williams|                   0|                     0|    5|1336003200|Wonderful, tasty ...|This taffy is so ...|\n",
            "|  9|B000E7L2R4|A1MZYO9TZK0BBI|            R. James|                   1|                     1|    5|1322006400|          Yay Barley|Right now I'm mos...|\n",
            "| 10|B00171APVA|A21BT40VZCCYT4|       Carol A. Reed|                   0|                     0|    5|1351209600|    Healthy Dog Food|This is a very he...|\n",
            "| 11|B0001PB9FE|A3HDKO7OW0QNK4|        Canadian Fan|                   1|                     1|    5|1107820800|The Best Hot Sauc...|I don't know if i...|\n",
            "| 12|B0009XLVG0|A2725IB4YY9JEB|\"A Poeng \"\"Sparky...|                   4|                     4|    5|1282867200|\"My cats LOVE thi...|One of my boys ne...|\n",
            "| 13|B0009XLVG0| A327PCT23YH90|                  LT|                   1|                     1|    1|1339545600|My Cats Are Not F...|My cats have been...|\n",
            "| 14|B001GVISJM|A18ECVX2RJ7HUE| \"willie \"\"roadie\"\"\"|                   2|                     2|    4|1288915200|   fresh and greasy!|good flavor! thes...|\n",
            "| 15|B001GVISJM|A2MUGFV2TDQ47K|\"Lynrie \"\"Oh HELL...|                   4|                     5|    5|1268352000|Strawberry Twizzl...|The Strawberry Tw...|\n",
            "| 16|B001GVISJM|A1CZX3CP8IKQIJ|        Brian A. Lee|                   4|                     5|    5|1262044800|Lots of twizzlers...|My daughter loves...|\n",
            "| 17|B001GVISJM|A3KLWF6WQ5BNYO|      Erica Neathery|                   0|                     0|    2|1348099200|          poor taste|I love eating the...|\n",
            "| 18|B001GVISJM| AFKW14U97Z6QO|               Becca|                   0|                     0|    5|1345075200|            Love it!|I am very satisfi...|\n",
            "| 19|B001GVISJM|A2A9X58G2GTBLP|             Wolfee1|                   0|                     0|    5|1324598400|  GREAT SWEET CANDY!|Twizzlers, Strawb...|\n",
            "| 20|B001GVISJM|A3IV7CL2C13K2U|                Greg|                   0|                     0|    5|1318032000|Home delivered tw...|Candy was deliver...|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df1"
      ],
      "metadata": {
        "id": "M_tSizm8jqhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mengecek dan menghapus missing value"
      ],
      "metadata": {
        "id": "b1YE0NURiJEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variable_name=\"Text\"\n",
        "# Menghitung jumlah missing value pada variabel tertentu\n",
        "missing_count = df.select(variable_name) \\\n",
        "    .where(col(variable_name).isNull() | isnan(variable_name)) \\\n",
        "    .count()\n",
        "\n",
        "# Menampilkan jumlah missing value\n",
        "print(\"Jumlah missing value pada variabel\", variable_name, \"adalah\", missing_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKDrBDNziNFy",
        "outputId": "22bbf606-f5b7-4d6e-927e-1e59763084d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah missing value pada variabel Text adalah 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PquUyjAMhmfj",
        "outputId": "15c352cc-17a6-4a27-e5bf-767b5c886f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568444"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mengecek tipe datanya. kalo masi string, diubah dulu ke numerik"
      ],
      "metadata": {
        "id": "YjAqFbVliYtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mengecek tipe data\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APtmUD4Ahc-9",
        "outputId": "b86c769b-e490-48fd-b2d8-9121a54d021c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: integer (nullable = true)\n",
            " |-- ProductId: string (nullable = true)\n",
            " |-- UserId: string (nullable = true)\n",
            " |-- ProfileName: string (nullable = true)\n",
            " |-- HelpfulnessNumerator: string (nullable = true)\n",
            " |-- HelpfulnessDenominator: string (nullable = true)\n",
            " |-- Score: string (nullable = true)\n",
            " |-- Time: string (nullable = true)\n",
            " |-- Summary: string (nullable = true)\n",
            " |-- Text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType\n",
        "# Mengubah tipe data kolom \"score\" menjadi integer\n",
        "df = df.withColumn(\"Score\", col(\"Score\").cast(IntegerType()))\n"
      ],
      "metadata": {
        "id": "oq46mBO1k91I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Menentukan label sentimen berdasarkan score nya"
      ],
      "metadata": {
        "id": "TiX81JUjioxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "df=df.filter((f.col('Score')!=3))\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ron7ozJoK2S",
        "outputId": "fc41c12e-90c1-4a83-e93c-80ed213cd935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "525660"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Definisikan fungsi untuk menentukan label sentimen\n",
        "def get_sentiment_label(score):\n",
        "    if score > 3:\n",
        "        return \"positif\"\n",
        "    elif score < 3:\n",
        "        return \"negatif\"\n",
        "    else:\n",
        "        return \"netral\"\n",
        "\n",
        "# Daftarkan fungsi sebagai User Defined Function (UDF) pada PySpark\n",
        "sentiment_udf = udf(get_sentiment_label, StringType())\n",
        "\n",
        "# Menerapkan UDF pada kolom \"score\" dalam DataFrame\n",
        "df = df.withColumn(\"sentiment\", sentiment_udf(\"Score\"))\n",
        "df.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osKewVg3i9wM",
        "outputId": "a83f79de-59df-43cf-fb41-c0de3a1b6f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+\n",
            "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|sentiment|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+\n",
            "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|  positif|\n",
            "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|  negatif|\n",
            "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|  positif|\n",
            "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|  negatif|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### kolom sentimen, dijadiin biner 0 dan 1"
      ],
      "metadata": {
        "id": "LS2lieo0jAHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def toBinary(score):\n",
        "    if score >= 3: return 1\n",
        "    else: return 0\n",
        "udfScoretoBinary=udf(toBinary, StringType())\n",
        "\n",
        "df = df.withColumn(\"Target\", udfScoretoBinary(\"Score\"))\n",
        "print(df.show(4))\n",
        "print(df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7VhIix-lzst",
        "outputId": "f2f2670b-2b6a-4ccb-bdaa-b23136d128e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+------+\n",
            "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|sentiment|Target|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+------+\n",
            "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|  positif|     1|\n",
            "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|  negatif|     0|\n",
            "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|  positif|     1|\n",
            "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|  negatif|     0|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+------+\n",
            "only showing top 4 rows\n",
            "\n",
            "None\n",
            "525660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing"
      ],
      "metadata": {
        "id": "G0l2yb6rjTzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3fQ1olem4ek",
        "outputId": "1b8c147c-de47-4382-8694-bef8535f001d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEXT Pre-processing\n",
        "\n",
        "##COnvert to lower\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "lower_udf =udf(lower,StringType())\n",
        "\n",
        "\n",
        "##Remove nonAscii\n",
        "def strip_non_ascii(data_str):\n",
        "#''' Returns the string without non ASCII characters'''\n",
        "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
        "    return ''.join(stripped)\n",
        "# setup pyspark udf function\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
        "\n",
        "##FIx abbreviations\n",
        "def fix_abbreviation(data_str):\n",
        "    data_str = data_str.lower()\n",
        "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
        "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
        "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
        "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
        "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
        "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
        "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
        "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
        "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
        "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
        "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
        "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
        "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
        "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
        "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
        "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
        "    data_str = re.sub(r'rt\\b', '', data_str)\n",
        "    data_str = data_str.strip()\n",
        "    return data_str\n",
        "\n",
        "##Remove punctuations mentions and alphanumeric characters\n",
        "def remove_features(data_str):\n",
        "# compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "# convert to lowercase\n",
        "    data_str = data_str.lower()\n",
        "# remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "# remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "# remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "# remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "# remove non a-z 0-9 characters and words shorter than 1 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word):\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word):\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "# remove unwanted space, *.split() will automatically split on\n",
        "# whitespace and discard duplicates, the \" \".join() joins the\n",
        "# resulting list into one string.\n",
        "    return \" \".join(cleaned_str.split())\n",
        "# setup pyspark udf function\n",
        "\n",
        "##Remove stop words\n",
        "def stopwords_delete(word_list):\n",
        "        from nltk.corpus import stopwords\n",
        "        filtered_words=[]\n",
        "        return word_list\n",
        "\n",
        "# Part-of-Speech Tagging\n",
        "def tag_and_remove(data_str):\n",
        "    cleaned_str = ' '\n",
        "# noun tags\n",
        "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
        "# adjectives\n",
        "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
        "# verbs\n",
        "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
        "# break string into 'words'\n",
        "    text = data_str.split()\n",
        "# tag the text and keep only those with the right tags\n",
        "    tagged_text = pos_tag(text)\n",
        "    for tagged_word in tagged_text:\n",
        "        if tagged_word[1] in nltk_tags:\n",
        "            cleaned_str += tagged_word[0] + ' '\n",
        "    return cleaned_str\n",
        "\n",
        "##Lemmatization\n",
        "def lemmatize(data_str):\n",
        "# expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str\n"
      ],
      "metadata": {
        "id": "9OSD5HIel_19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_udf =udf(lower,StringType())\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
        "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "remove_stops_udf = udf(stopwords_delete, StringType())\n",
        "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
        "lemmatize_udf = udf(lemmatize, StringType())"
      ],
      "metadata": {
        "id": "a6ZvQZElmgKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"lower_text\",lower_udf(df[\"Text\"]))\n",
        "df = df.withColumn(\"text_non_asci\",fix_abbreviation_udf(df[\"lower_text\"]))\n",
        "df = df.withColumn(\"fixed_abbrev\",fix_abbreviation_udf(df[\"text_non_asci\"]))\n",
        "df = df.withColumn('removed_features',remove_features_udf(df['fixed_abbrev']))\n",
        "df.show(5,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiktG-TOmqTR",
        "outputId": "9323e038-a325-4626-e82e-14621709fb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+------+--------------------+--------------------+--------------------+--------------------+\n",
            "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|sentiment|Target|          lower_text|       text_non_asci|        fixed_abbrev|    removed_features|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|  positif|     1|i have bought sev...|i have bought sev...|i have bought sev...|i have bought sev...|\n",
            "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|  negatif|     0|\"product arrived ...|\"product arrived ...|\"product arrived ...|product arrived l...|\n",
            "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|  positif|     1|\"this is a confec...|\"this is a confec...|\"this is a confec...|this is a confect...|\n",
            "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|  negatif|     0|if you are lookin...|if you are lookin...|if you are lookin...|if you are lookin...|\n",
            "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|  positif|     1|great taffy at a ...|great taffy at a ...|great taffy at a ...|great taffy at a ...|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+---------+------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_stop_words = df.withColumn(\"stopwords_delete\", remove_stops_udf(\"removed_features\")).select('Text','stopwords_delete','Target')\n",
        "df_no_stop_words.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgOTr7jrmt4K",
        "outputId": "453f6e03-b5b7-4a12-91e2-b8afdf0d912e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+\n",
            "|                Text|    stopwords_delete|Target|\n",
            "+--------------------+--------------------+------+\n",
            "|I have bought sev...|i have bought sev...|     1|\n",
            "|\"Product arrived ...|product arrived l...|     0|\n",
            "|\"This is a confec...|this is a confect...|     1|\n",
            "|If you are lookin...|if you are lookin...|     0|\n",
            "|Great taffy at a ...|great taffy at a ...|     1|\n",
            "+--------------------+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pos_tagging=df_no_stop_words.withColumn(\"tag_and_remove_pos\", tag_and_remove_udf(\"stopwords_delete\")).select('Text','tag_and_remove_pos','Target')"
      ],
      "metadata": {
        "id": "-yZLA5keTIpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QinCuyw6lN76",
        "outputId": "2eb1f462-dee4-4082-b03f-f967bf13dbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing the document\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"tag_and_remove_pos\", outputCol=\"words\")\n",
        "wordsDataFrame = tokenizer.transform(df_pos_tagging)\n",
        "for words_label in wordsDataFrame.select(\"words\", \"Target\").take(3):\n",
        "    print(words_label)\n",
        "\n",
        "df_text = df.withColumn(\"text_lower\",lower_udf(df[\"Text\"])).select('text_lower','Target')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Wn-nlsk4SX",
        "outputId": "570a688b-1eee-4b59-b213-7f31cddbf151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(words=['', 'i', 'have', 'bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'have', 'found', 'be', 'good', 'quality', 'product', 'looks', 'stew', 'processed', 'meat', 'smells', 'labrador', 'is', 'finicky', 'appreciates', 'product', 'better', 'most'], Target='1')\n",
            "Row(words=['', 'product', 'arrived', 'labeled', 'jumbo', 'salted', 'peanuts', 'peanuts', 'were', 'small', 'sized', 'unsalted', 'sure', 'was', 'error', 'vendor', 'intended', 'represent', 'product', 'jumbo'], Target='0')\n",
            "Row(words=['', 'is', 'confection', 'has', 'been', 'few', 'centuries', 'is', 'light', 'pillowy', 'citrus', 'gelatin', 'nuts', 'case', 'filberts', 'is', 'cut', 'tiny', 'squares', 'coated', 'powdered', 'sugar', 'is', 'tiny', 'mouthful', 'heaven', 'chewy', 'flavorful', 'i', 'recommend', 'yummy', 'treat', 'are', 'familiar', 'story', 'c', 's', 'lewis', 'lion'], Target='1')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filtered\")\n",
        "wordsDataFrame1 = remover.transform(wordsDataFrame).select(\"Target\",\"words_filtered\")\n",
        "wordsDataFrame1.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc7KTnu2lBDr",
        "outputId": "85b227dc-d1fc-4c36-c490-35e7a7fbb013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|Target|      words_filtered|\n",
            "+------+--------------------+\n",
            "|     1|[, bought, severa...|\n",
            "|     0|[, product, arriv...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYJIQIdaloFx",
        "outputId": "1e53c185-a710-4062-bf27-8cafb30a2e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_text_lemma=df_pos_tagging.withColumn(\"lemmatized_text\",lemmatize_udf(\"tag_and_remove_pos\")).select('Text','lemmatized_text','Target')\n",
        "df_text_lemma.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2_tBnjslU8R",
        "outputId": "44bf18d6-8934-4425-f196-ea0340e09d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+\n",
            "|                Text|     lemmatized_text|Target|\n",
            "+--------------------+--------------------+------+\n",
            "|I have bought sev...|i have buy severa...|     1|\n",
            "|\"Product arrived ...|product arrive la...|     0|\n",
            "|\"This is a confec...|be confection hav...|     1|\n",
            "|If you are lookin...|be look secret in...|     0|\n",
            "|Great taffy at a ...|great taffy great...|     1|\n",
            "+--------------------+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "# Create Unique ID\n",
        "df_text_lemma = df_text_lemma.withColumn(\"uid\", monotonically_increasing_id())\n",
        "df_text_lemma.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81KA0Kpli7X",
        "outputId": "d3612f19-ce44-47a3-faf2-352ae5df9ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+---+\n",
            "|                Text|     lemmatized_text|Target|uid|\n",
            "+--------------------+--------------------+------+---+\n",
            "|I have bought sev...|i have buy severa...|     1|  0|\n",
            "|\"Product arrived ...|product arrive la...|     0|  1|\n",
            "|\"This is a confec...|be confection hav...|     1|  2|\n",
            "|If you are lookin...|be look secret in...|     0|  3|\n",
            "+--------------------+--------------------+------+---+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing data for modelling"
      ],
      "metadata": {
        "id": "c1ey6fKMl6lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = df_text_lemma.select('uid', 'lemmatized_text','Target')\n",
        "#data=wordsDataFrame2\n",
        "data.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQbcLDfylxsl",
        "outputId": "7dbf24a3-d19f-4dd1-b463-a54ce066777e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+------+\n",
            "|uid|     lemmatized_text|Target|\n",
            "+---+--------------------+------+\n",
            "|  0|i have buy severa...|     1|\n",
            "|  1|product arrive la...|     0|\n",
            "|  2|be confection hav...|     1|\n",
            "|  3|be look secret in...|     0|\n",
            "+---+--------------------+------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Membagi data training dan testing"
      ],
      "metadata": {
        "id": "ioYD13NPmFmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "j5K_nWHSl_4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching the RDD for training\n",
        "trainingData\n",
        "#Renaming features for modeling\n",
        "training = trainingData.selectExpr(\"lemmatized_text as text\", \"Target as label\")\n",
        "training = training.withColumn(\"label\", training[\"label\"].cast(DoubleType()))"
      ],
      "metadata": {
        "id": "eEe5TJH9mKrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching the RDD for test\n",
        "testData\n",
        "#Renaming features for modeling\n",
        "test = testData.selectExpr(\"lemmatized_text as text\", \"Target as label\")\n",
        "test = test.withColumn(\"label\", test[\"label\"].cast(DoubleType()))"
      ],
      "metadata": {
        "id": "J1RKBWbNmRGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the Training set anc creating TF-IDF matrix using HashingTF"
      ],
      "metadata": {
        "id": "STamORCXmV0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"hashing\")\n",
        "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")"
      ],
      "metadata": {
        "id": "gsr3uK0imTKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-7XSGx3qiBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling dan Evaluasi"
      ],
      "metadata": {
        "id": "VeRIfYnPmr78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Model"
      ],
      "metadata": {
        "id": "ZwQxv-om3oyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
        "# Training the model\n",
        "model = pipeline.fit(training)\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "print(\"Waktu pelatihan model: \", training_time, \" detik\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4uTKoTxmv70",
        "outputId": "38a148b5-e1d3-46be-8f69-9d056c0e12ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waktu pelatihan model:  6021.695109605789  detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicing Output\n",
        "prediction = model.transform(test)"
      ],
      "metadata": {
        "id": "dz3itmAZnV1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.select(\"label\", \"prediction\").show(10,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB7YmvoiYgEQ",
        "outputId": "c1a70af3-0dfe-4b7f-b599-ae0b1e1d394e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "|1.0  |1.0       |\n",
            "+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.printSchema()"
      ],
      "metadata": {
        "id": "yiLNzSbhYlLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a889ae-0963-4df7-b3ee-a0db02f0f0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: double (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- hashing: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(prediction)"
      ],
      "metadata": {
        "id": "wImmtKp7YrJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188762a8-c0c4-4daa-e63f-9db043b0e49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9152259959378706"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "3Jqn4tncYwPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
        "pipeline_nb = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
        "# Training the model\n",
        "model_nb = pipeline_nb.fit(training)"
      ],
      "metadata": {
        "id": "N6kbY3hXYu9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicing Output\n",
        "prediction_nb = model_nb.transform(test)"
      ],
      "metadata": {
        "id": "ptUFwCpGYzlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_nb.printSchema()"
      ],
      "metadata": {
        "id": "pB15bwjTY_lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26633c08-2a22-4b26-ee3f-1f860b7cda56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: double (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- hashing: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator_nb = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_nb.evaluate(prediction_nb)"
      ],
      "metadata": {
        "id": "VqO5oKDaZFBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24509f97-62d7-4092-8f6c-0849270d3599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8783411775648421"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTW_RSSbcser"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}